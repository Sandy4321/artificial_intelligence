---
title: "Untitled"
author: "Paul Oldham"
date: "21/05/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## General Trends data

Note the below is kind of working but needs more work. 

Looking up general trends data

```{r}
trends <- trends_available(token = NULL, parse = TRUE)
```

To set up a database to store and retrieve tweets every hour

```{r}
library(DBI)
library(RSQLite)
library(rtweet) # mkearney/rtweet

repeat {
  message("Retrieving AI tweets...") # optional
  ai <- stream_tweets("#AI", parse = TRUE)
  db_con <- dbConnect(RSQLite::SQLite(), "ai_tweets.db")
  dbWriteTable(db_con, "ai_tweets", ai, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist
  dbDisconnect(db_con)
  Sys.sleep(10 * 60) # sleep for 10 minutes
}
```

```{r}
repeat {
  message("Retrieveing trends...") # optional
  us <- get_trends("united states")
  db_con <- dbConnect(RSQLite::SQLite(), "us-trends.db")
  dbWriteTable(db_con, "us_trends", us, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist
  dbDisconnect(db_con)
  Sys.sleep(10 * 60) # sleep for 10 minutes
}
```


```{r}
stream_tweets("#AI",
  timeout = (60 * 10),
  parse = FALSE,
  file_name = "tweets3" # stores as json
)
stream_tweets("#AI",
  timeout = (60 * 10),
  parse = FALSE,
  file_name = "tweets4"
)
```

parse the results

```{r}
tw1 <- parse_stream("tweets1.json")
tw2 <- parse_stream("tweets2.json")
tw3
tw4
```

The above needs a different set up

stream tweets for a minute once an hour for 24 hours time 7 days a week
```{r}
stream_tweets("#AI",
  timeout = (60 * 60 * 24 * 7),
  parse = FALSE,
  file_name = "aitweetsweek.json" # stores as json
)
```

