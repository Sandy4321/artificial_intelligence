---
title: "Untitled"
author: "Paul Oldham"
date: "16/04/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## AI and machine learning tweets

Install and authenticate rwteet process

Create a starter set from the AI hashtag to identify other hashtags

```{r}
library(rtweet)
AI <- search_tweets(
  "#AI", n = 10000, include_rts = FALSE
)
```

What hashtags are people using in association with AI
```{r}
library(tidytext)
ai_hashtags <- AI %>%
  unnest(hashtags)
```




```{r}
# divide into words and remove stopwords
ai_tokens <- AI %>%
  unnest_tokens(., word, text) %>% 
  anti_join(stop_words)

# join to sentiments dictionary (in this case bing)
ai_sentiment <- ai_tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

# What are the top positive terms in the tweets

ai_sentiment %>% 
  arrange(sentiment) %>% 
  View()

# Wnat are the top ranking negative terms
ai_sentiment %>% 
  dplyr::filter(sentiment <= 0) %>%
  arrange(sentiment) %>% 
  View()

# This is a very raw quick look at sentiments to see what comes up
# No banana yet
ai_sentiment %>% 
  count(word, sentiment, sort = TRUE) %>% ungroup() %>% 
  group_by(sentiment) %>%
  dplyr::top_n(10) %>%
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()



```



Looking up general trends data

```{r}
trends <- trends_available(token = NULL, parse = TRUE)
```

To set up a database to store and retrieve tweets every hour

```{r}
library(DBI)
library(RSQLite)
library(rtweet) # mkearney/rtweet

repeat {
  message("Retrieving AI tweets...") # optional
  ai <- stream_tweets("#AI", parse = TRUE)
  db_con <- dbConnect(RSQLite::SQLite(), "ai_tweets.db")
  dbWriteTable(db_con, "ai_tweets", ai, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist
  dbDisconnect(db_con)
  Sys.sleep(10 * 60) # sleep for 10 minutes
}
```

```{r}
repeat {
  message("Retrieveing trends...") # optional
  us <- get_trends("united states")
  db_con <- dbConnect(RSQLite::SQLite(), "us-trends.db")
  dbWriteTable(db_con, "us_trends", us, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist
  dbDisconnect(db_con)
  Sys.sleep(10 * 60) # sleep for 10 minutes
}
```






```{r}
stream_tweets("#AI",
  timeout = (60 * 10),
  parse = FALSE,
  file_name = "tweets3" # stores as json
)
stream_tweets("#AI",
  timeout = (60 * 10),
  parse = FALSE,
  file_name = "tweets4"
)
```

parse the results

```{r}
tw1 <- parse_stream("tweets1.json")
tw2 <- parse_stream("tweets2.json")
tw3
tw4
```



The above needs a different set up

stream tweets for a minute once an hour for 24 hours time 7 days a week
```{r}
stream_tweets("#AI",
  timeout = (60 * 60 * 24 * 7),
  parse = FALSE,
  file_name = "aitweetsweek.json" # stores as json
)
```

